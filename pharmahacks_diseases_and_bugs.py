# -*- coding: utf-8 -*-
"""Pharmahacks - Diseases and Bugs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nw0hZZMnqUbYrmcQitEsLnIHdupn6ax8

# Preamble
"""

import pandas as pd
import numpy as np

"""# Preliminary Exploration"""

data = pd.read_csv("/dataset.csv", index_col = "Sample")
data.head()

print(data.shape)

# Groups data by parameter categroy
def group(data, param):
  res = data.groupby(param)    
  res = [res.get_group(x) for x in res.groups]
  return res
  
disease_groups = group(data,'disease')

for grp in disease_groups:
  print(pd.unique(grp['disease']))

"""## Disease 1"""

disease1 = disease_groups[0]
print(disease1.shape)

disease1.describe()

"""## Disease 2"""

disease2 = disease_groups[1]
print(disease2.shape)

disease_groups[1].describe()

"""## Disease 3

"""

disease3 = disease_groups[2]
print(disease3.shape)

disease3.describe()

"""## Healthy"""

healthy = disease_groups[3]
healthy.head()

"""## Length Diffs

This section was computed to allow us to visualize the imbalance of the dataset.
"""

disease_cols = ['Disease-1', 'Disease-2', 'Disease-3', 'Healthy']
size_sum = pd.DataFrame([disease1.shape[0], disease2.shape[0], disease3.shape[0], healthy.shape[0]], columns=['Length'])
size_sum.index = disease_cols
size_sum.head()

disease_cols

"""# Machine Learning

In this section, we undertake the machine learning section of the challenge.

## Part 1: Preprocessing

To first make our data much more usable and representative, we first normalized the data before performing a $\log$ transform. This helped minimize the affects of the outliers in our analysis.
"""

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split
import regex as re

# Gathering important labels
sample_idx = data.index
bacteria_headers = data.columns[0:-1]

# Changing labels to numerical, correspondance --
# 0: Healthy, 1: Disease 1, 2: Disease 2, 3: Disease 3
diagnosis_labels = data['disease']
diagnosis_labels = [re.sub('Disease-','', str(x)) for x in diagnosis_labels]
diagnosis_labels = [re.sub('Healthy','0', str(x)) for x in diagnosis_labels]
diagnosis_labels = pd.to_numeric(diagnosis_labels)

# Gathering data
norm_data = pd.DataFrame(np.log(data.loc[:,"Bacteria-1":"Bacteria-1094"] + 1), index = sample_idx, columns = bacteria_headers)
norm_data = pd.DataFrame(normalize(norm_data.loc[:,"Bacteria-1":"Bacteria-1094"]), index = sample_idx, columns = bacteria_headers)


norm_data['disease'] = diagnosis_labels

norm_data.head()

"""### Undersampling
Because of the great deal of imbalance in our dataset, we wanted to resample to ensure that the dataset is able to be accurate and unbiased towards the more prevelant 'healthy' labels. 

We have tested oversampling, but have noticed that it took too long for some fitting.
"""

rus = RandomUnderSampler(random_state=537)
us_data, us_cfx = rus.fit_resample(norm_data.drop('disease', axis=1), diagnosis_labels)

# ros = RandomOverSampler(random_state=537)
# os_data, os_cfx = ros.fit_resample(norm_data.drop('disease', axis=1), diagnosis_labels)

# check 
for i in range(0,4):
  print(len(us_cfx[us_cfx == i])) 
  # print(len(os_cfx[os_cfx == i]))

us_data.head()

"""### Splitting Test/Train

"""

X_train, X_test, y_train, y_test = train_test_split(us_data, us_cfx, test_size=0.2, random_state=268)

"""## Part 2: Testing Various Models
In this section, we test out various ML models to see which is best suited to fit our data.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
def model_score(y, y_pred):
  print("Accuracy: ", accuracy_score(y, y_pred))
  print("Precision: ", precision_score(y, y_pred, average='weighted'))
  print("Recall: ", recall_score(y, y_pred, average='weighted'))
  print("f1: ", f1_score(y, y_pred, average='weighted'))
  return

"""### N-Nearest Neighbours"""

from sklearn.neighbors import NeighborhoodComponentsAnalysis, KNeighborsClassifier
from sklearn.pipeline import Pipeline

nca = NeighborhoodComponentsAnalysis(random_state=789)
knn = KNeighborsClassifier(n_neighbors=10)
pipe = Pipeline([('nca', nca), ('knn', knn)])
pipe.fit(X_train, y_train)

pred = pipe.predict(X_test)
model_score(y_test, pred)

"""### MLPC

We found MLPC to have the greatest success in providing a highier level of accuracy. We could improve this better by increase the number of iterations to minimize the residue at the cost of computation time. Otherwise, it would have been beneficial if we were able to have more data to improve the model, especially as it was limited to the size of the 'Disease-2'.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV

nn_us = MLPClassifier(activation='tanh', solver='adam', learning_rate='invscaling', max_iter = 200)
nn_model_us = nn_us.fit(X_train, y_train)

nn_pred_us = nn_model_us.predict(X_test)
model_score(y_test, nn_pred_us)

"""Testing another version..."""

nn_us = MLPClassifier(activation='tanh', solver='adam', learning_rate='constant', max_iter = 200)
nn_model_us = nn_us.fit(X_train, y_train)

nn_pred_us = nn_model_us.predict(X_test)
model_score(y_test, nn_pred_us)

import joblib
joblib.dump(nn_model_us, 'nn_model.pkl')

"""## Hyperparam Tuning
We also wanted to see if we could tune our model a little more, just to get the best of our scores. Here we investigate just that and discovered that a MLPC none of these alternative had beat the previous models. We could do a wider grid-search but we were limited by time. 
"""

parameters = {'activation' : ['logistic'],
              'solver' : ['adam', 'lbfgs'],
              'learning_rate' : ['constant', 'invscaling']}

mlpc = MLPClassifier()
clf = GridSearchCV(mlpc, parameters)
clf.fit(X_train, y_train)

pd.DataFrame(clf.cv_results_)

"""### Alternate

We tested an alternate classification with a different approach to pre-processing wherein we only took a log transform and did not scale the data. This resulted in improved scores, but we did not choose this as it may be baised towards the larger sets of data eg 'Healthy' diagnosis.
"""

df = data.loc[:,"Bacteria-1":"Bacteria-1094"];
logtrans = np.log(df + 1)

X_train, X_test, y_train, y_test = train_test_split(logtrans, diagnosis_labels, test_size=0.2, random_state=221)

nn = MLPClassifier(activation='logistic', solver='lbfgs', learning_rate='adaptive', max_iter = 2000)
nn_model = nn.fit(X_train, y_train)

nn_pred = nn.predict(X_test)
model_score(y_test, nn_pred)